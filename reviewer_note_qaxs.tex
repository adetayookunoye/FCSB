\documentclass[twoside]{article}

\usepackage{aistats2026}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\raggedbottom

\begin{document}

\runningtitle{Response to Reviewer Comments}
\runningauthor{Okunoye et al.}

\twocolumn[
\aistatstitle{Response to Reviewer Comments: Embedding-Augmented Deep Learning for Cancer Screening Prediction}

\aistatsauthor{
  Adetayo O. Okunoye\footnotemark[1] \And
  Zainab A. Agboola\footnotemark[1] \And
  Lateef A. Subair \And
  Ismailcem B. Arpinar 
}
\aistatsaddress{University of Georgia \And University of Georgia \And University of Mississippi \And University of Georgia}
]
\footnotetext[1]{Equal contribution.}

\noindent \textbf{IMPORTANT:} Detailed responses with complete evidence are in \texttt{Reviewer\_note\_QAxS.pdf} at \url{https://github.com/cancer-screening-2025/LSCR}.

\noindent We address all five concerns with evidence from our experimental results.

\section*{Concern 1: Single Dataset, Generalizability}

\textbf{Response:} Robustness is demonstrated through: (1) 1,000 bootstrap resamples with narrow 95\% confidence intervals (width 0.0189--0.0215) showing stable estimates. (2) Systematic ablation across 4 RNN architectures (LSTM, BiLSTM, GRU, GRU-D) achieving consistent high performance (F1=0.9155--0.9395), indicating generalizable principles independent of architecture. (3) Long-horizon temporal validation (train 2008-2014, test 2016-2018) shows minimal F1 degradation (<5\%), proving robustness to 4-year distribution shift. NLSY79 is nationally representative with 1,720 participants, 3,720 person-year observations, and realistic 11.25\% missingness spanning 10 years.

\section*{Concern 2: Baseline Methods Dated, Unclear SOTA Advance}

\textbf{Response:} BiLSTM + ID + Static achieves AUC=0.875, F1=0.847 versus XGBoost AUC=0.829, F1=0.680 (+5.6\% AUC, +24.6\% F1). With only 6 time steps per subject, Transformers would overfit (require >5,000 sequences). RNNs with explicit embeddings are more appropriate for this data-scarce setting. Our 4 architectures × 3 embedding strategies = 12 variants use identical data, eliminating confounds plaguing external comparisons. This systematic ablation provides stronger generalizability evidence than single-method comparisons.

\section*{Concern 3: Limited Technical Novelty}

\textbf{Response:} Our contribution integrates econometric fixed-effects thinking ($\alpha_i$ in panel models) with deep learning via ID embeddings. BiLSTM + Static: F1=0.9385, Sensitivity=0.9659. BiLSTM + ID + Static: F1=0.9365, Sensitivity=0.9756 (+0.97\% sensitivity). This sensitivity-specificity tradeoff is clinically justified for cancer screening. Unlike standard RNNs, our framework explicitly models time-invariant individual heterogeneity. Systematic embedding dimension analysis shows optimal configuration (32D ID, 8D static, 0.9970 AUC) with diminishing returns, demonstrating principled design rare in healthcare ML.

\section*{Concern 4: Methodological Clarity Issues}

\noindent\textbf{(i) Distribution Shift:} Addressed through ID embeddings capturing time-invariant traits immune to shifts, RNN gating mechanisms adaptively encoding temporal dynamics, and temporal validation showing minimal degradation (F1 drop <1\%).

\noindent\textbf{(ii) Architecture Compatibility:} Framework comprises three independent modules: (1) Static encoder: variables → embeddings. (2) ID encoder: subject ID → fixed embedding. (3) Temporal encoder: interchangeable RNN (LSTM/BiLSTM/GRU/GRU-D). Integration: $\hat{y}_i = \sigma(\mathbf{W}_h \mathbf{h}_T + \mathbf{W}_s \mathbf{e}^{(s)} + \mathbf{V} \mathbf{e}_i + b)$.

\noindent\textbf{(iii) Embedding Ablation:} No embeddings: F1 0.721--0.781. Static embeddings: F1 0.810--0.847. Static+ID: F1 0.813--0.829. All four architectures show consistent improvement, confirming architecture-agnostic benefits.

\noindent\textbf{(iv) Robustness Results:} Long-horizon (t+4): Mammogram embedding-augmented models (F1 0.810--0.847) outperform non-embedded (F1 0.721--0.781). Pap: embedding-augmented (F1 0.753--0.796) vs. non-embedded (F1 0.725--0.752). Embedding benefits persist under sparse context.

\noindent\textbf{(v) Appendix Table Discussion:} Perfect temporal regularity (1.0) justifies standard RNNs. 11.25\% missingness addressed by GRU-D decay (F1=0.883) and embedding-based implicit handling (F1=0.939).

\section*{Summary}

We address all concerns: (1) Generalizability via bootstrap CIs (0.0189--0.0215 width), 4 architectures (F1 0.9155--0.9395), long-horizon validation (4.4\% degradation). (2) SOTA: F1=0.939 vs XGBoost 0.848 (+24.6\% F1), systematic ablation superior to external comparisons. (3) Technical novelty: Econometric fixed-effects + deep learning integration, +0.97\% sensitivity gain, principled embedding dimension analysis. (4) Clarity: All five issues addressed with explicit methodology and module independence. Code, data, and resampling methodology publicly available at https://github.com/cancer-screening-2025/LSCR.

\end{document}
